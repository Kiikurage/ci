# 2013-夏

## 1

### (1)

$$
\begin{align}
w_i^{(t+1)}
&= \prod_{j=1}^{t+1}P_i^{(j)} \\
&= P_i^{(t+1)} \cdot \prod_{j=1}^{t}P_i^{(j)} \\
&= P_i^{(t+1)} w_i^{(t)}
\end{align}
$$

$$
\begin{align}
v_i^{(t+1)}
&= \sum_i v_i \frac{w_i^{(t)}}{\sum_i w_i^{(t)}} \\
&= 1 \cdot \frac{P_i^{(t)} w_i^{(t-1)}}{\sum_i P_i^{(t)} w_i^{(t-1)}} \\
&= \frac{P_i^{(t)} w_i^{(t-1)}}{\sum_i P_i^{(t)} w_i^{(t-1)}}
\end{align}
$$

### (2)

$$ v^{(t)} $$の比と$$ w^{(t-1)} $$の比が等しいことから、

$$
\begin{align}
v_i^{(t+1)}
&= \frac{P_i^{(t)} w_i^{(t-1)}}{\sum_i P_i^{(t)} w_i^{(t-1)}} \\
&= \frac{P_i^{(t)} v_i^{(t)}}{\sum_i P_i^{(t)} v_i^{(t)}} \\
&= \frac{P_i^{(t)}}{\hat{P}^{(t)}} v_i^{(t)} \\
\end{align}
$$

が成立する。よってアルゴリズムは次のようになる。

**step 1**

各予報士の重み$$ v_i^{(t)} $$と、各予報士の予報$$ P_i^{(t)} $$が与えられる。重み付き総和を取り、$$ \hat{P}^{(t)} $$を出力する

**step 2**

得られた$$ \hat{P}^{(t)} $$を用いて

$$
v_i^{(t+1)} = \frac{P_i^{(t)}}{\hat{P}^{(t)}} v_i^{(t)}
$$

と更新する。

このアルゴリズムの計算量は、時間計算量が$$ O(N) $$、空間計算量が$$ O(N) $$となる。マルコフ連鎖により翌日の各予報士の信頼度・予測が求まるため、時間Tには依存しない。

### (3)

$$
\begin{align}
\log{\hat{P}^{(t)}}
&= \log{\biggl(\sum_i P_i^{(t)} v_i^{(t)}\biggr)} \\
&= \log{\biggl(\frac{1}{\sum_i w_i^{(t-1)}}\sum_i P_i^{(t)} w_i^{(t-1)}\biggr)} \\
&= \log{\biggl(\sum_i P_i^{(t)} w_i^{(t-1)}\biggr)} - \log{\sum_i w_i^{(t-1)}} \\
&= \log{\sum_i w_i^{(t)}} - \log{\sum_i w_i^{(t-1)}} \\
\therefore
\sum_{t=1}^T \biggl(-\log{\hat{P}^{(t)}} \biggr)
&= \log{\sum_i w_i^{(0)}} - \log{\sum_i w_i^{(T)}} \\
&= \log{N} - \log{\sum_i \prod_j P_i^{(j)}} \\
\therefore
Loss(x^T)
&= \log{N} - \log{\sum_i \prod_j P_i^{(j)}(x_j)}
\end{align}
$$

### (4)

累積予測損失が最小となった予報士を$$ i_0 $$番目の予報士とする。

この時、各予報士の累積予測損失に以下の関係が成り立つ。

$$
\begin{align}
\sum_{t=1}^{T} \bigl(-\log{P_{i}^{(t)}}\bigr)
&= -\log{\prod_{t=1}^{T} \bigl(P_{i}^{(t)}\bigr)} \\
&\ge -\log{\prod_{t=1}^{T} \bigl(P_{i_0}^{(t)}\bigr)} \\
\therefore
\log{\prod_{t=1}^{T} \bigl(P_{i}^{(t)}\bigr)}
&\le \log{\prod_{t=1}^{T} \bigl(P_{i_0}^{(t)}\bigr)} \\
\therefore
\prod_{t=1}^{T} \bigl(P_{i}^{(t)}\bigr)
&\le \prod_{t=1}^{T} \bigl(P_{i_0}^{(t)}\bigr)
\end{align}
$$

わからぬ

## 2
ぱす

## 3

### (1)
$$
\begin{align}
&(200+30+200+200) \cdot 0 \\
&+ (80+250+50+250) \cdot 1 \\
&+ 150\cdot(-4) \\
&= 0 + 630 - 600 = 30
\end{align}
$$

### (2)

- **(A):** 画素値の合計
- **(B):** 画素値を重みと見た時の重心
- **Y1-M00:** 12
- **Y1-M10:** 26
- **Y2-M00:** 11
- **Y2-M10:** 28

### (3)

**k=1:**
I4のサンプルが選ばれるので、「I」

**k=3:**
I4, C2, C4のサンプルが選ばれるので、「C」

### (4)

各サンプルの平均値は

$$ \bar{C} = (12, 25), \bar{I} = (10, 27) $$

であるから、識別境界の方程式は

$$ Y=X+15 $$

となる。よって、

$$
\begin{align}
Y>X+15&のとき、I \\
Y=X+15&のとき、識別不能 \\
Y<X+15&のとき、C
\end{align}
$$

と識別される。ゆえに、$$(13, 27)$$は「C」となる。

### (5)

k最近傍法では学習用サンプルに外れ値が含まれていた場合、その付近の識別精度は非常に小さくなる。言い換えると、学習用サンプル全体の情報を利用できず局所的な情報にとらわれてしまう。これは、「学習用サンプルが正しければ、その付近については少ない精度で非常に高い精度をだせる」ということの裏返しでもある。平均とのユークリッド距離による方法では、k最近傍法とは逆に、学習用サンプルが多くなければ精度は出ないが、学習用サンプルが多ければ、広い範囲において高い精度を出すことができ、外れ値の影響を受けにくい。ただし、この方法は境界面が直線の場合にしか適用できず、境界面が複雑な曲線となる場合は、境界面が直線となるような別の特徴空間へ写像するか、k最近傍法を用いる必要がある。
